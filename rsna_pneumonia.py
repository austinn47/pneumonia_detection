# -*- coding: utf-8 -*-
"""rsna_pneumonia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HKbE9Vzpp_bS37esbpITWHmDNzfuGcGE
"""

!pip install pydicom

!pip install utils

import glob, pylab, pandas as pd
import pydicom, numpy as np
import random
import json
import time
import copy
import pydicom
import torchvision
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
from torch.autograd import Variable
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt
from matplotlib import patches, patheffects

from sklearn.model_selection import train_test_split
from torchvision import datasets, transforms
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from torch.optim import lr_scheduler
from pathlib import Path

from sklearn.metrics import roc_auc_score

PATH = Path('/home/work/austin/Dataset/rsna-pneumonia-detection-challenge')
# read training lables
train_bb_df = pd.read_csv(PATH/'stage_2_train_labels.csv')
train_bb_df.head()

train_bb_df['duplicate'] = train_bb_df.duplicated(['patientId'], keep=False)
# see data
train_bb_df[train_bb_df['duplicate']].head()

detailed_df = pd.read_csv(PATH/'stage_2_detailed_class_info.csv')
# merge two df
class_df = train_bb_df.merge(detailed_df, on="patientId")
# class_df.head()
csv_df = class_df.filter(['patientId', 'Target'], )
csv_df = csv_df.set_index('patientId', )
# detailed_df.head() ,
class_df.head(10)
# csv_df.head()

class CDataset(Dataset):
    def __init__(self, ds, img_dir, class_df, transform=None, ext=None):
        self.ds = ds
        self.img_dir = img_dir
        self.class_df = class_df
        self.ext = ext or '.dcm'
        self.transform = transforms.Compose(transform) if transform else None

    def __len__(self):
        return len(self.ds)

    def read_dicom_image(self, loc):
        # return numpy array
        img_arr = pydicom.read_file(loc.as_posix()).pixel_array
        img_arr = img_arr/img_arr.max()
        img_arr = (255*img_arr).clip(0, 255).astype(np.uint8)
        img_arr = Image.fromarray(img_arr).convert('RGB') # model expects 3 channel image
        return img_arr

    def __getitem__(self, i):
        img = self.read_dicom_image(self.ds[i])
        if self.transform:
            img = self.transform(img)
        patientId = self.ds[i].name.split('.')[0]
        kls = self.class_df[self.class_df['patientId'] == patientId]
        return img, kls.iloc[0].Target

# # img_dir = PATH/'stage_1_train_images'
# img_dir = PATH/'stage_2_train_images'
# sample = random.sample(list(img_dir.iterdir()), 400) # sample
# # sample = list(img_dir.iterdir())
# train, test = train_test_split(sample)

# transform = [transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip() , transforms.ToTensor()]
# train_ds = CDataset(train, img_dir, class_df, transform=transform)
# test_ds = CDataset(test, img_dir, class_df, transform=transform)

# Define the path to the image directory
img_dir = PATH/'stage_2_train_images'

# Get the list of all images
all_images = list(img_dir.iterdir())

# Split the dataset into training (70%) and temp (30%)
train_images, test_images = train_test_split(all_images, test_size=0.15, random_state=42)


# Define transformations
transform = [transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip() , transforms.ToTensor()]

# Create datasets
train_ds = CDataset(train_images, img_dir, class_df, transform=transform)
test_ds = CDataset(test_images, img_dir, class_df, transform=transform)

batch_size=64
sz=224
train_dl = DataLoader(train_ds, batch_size=batch_size,)
test_dl = DataLoader(test_ds, batch_size=batch_size)

def show_img(im, figsize=None, ax=None):
    if not ax:
        fig,ax = plt.subplots(figsize=figsize)
    ax.imshow(im)
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
    return ax

def draw_outline(o, lw):
  o.set_path_effects([patheffects.Stroke(
      linewidth=lw, foreground='black'), patheffects.Normal()])

def draw_rect(ax, b):
    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor='white', lw=2))
    draw_outline(patch, 4)

def draw_text(ax, xy, txt, sz=14):
    text = ax.text(*xy, txt, verticalalignment='top', color='white', fontsize=sz, weight='bold')
    draw_outline(text, 1)

image, klass = next(iter(train_dl))
fig, axes = plt.subplots(1, 4, figsize=(12, 2))
for i,ax in enumerate(axes.flat):
    image, klass
#     ima=image[i].numpy().transpose((1, 2, 0))
    ima=image[i][0]
    b = klass[i]
    ax = show_img(ima, ax=ax)
    draw_text(ax, (0,0), b)

plt.tight_layout()

use_gpu = torch.cuda.is_available()
dataloaders = {'train': train_dl, 'val':test_dl}

def train_model(model, criterion, optimizer, scheduler, num_epochs=25):
    since = time.time()

    best_model_wts = copy.deepcopy(model.state_dict())
    best_auc = 0.0

    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch, num_epochs - 1))
        print('-' * 10)

        # Each epoch has a training and validation phase
        for phase in ['train', 'val']:
            if phase == 'train':
                scheduler.step()
                model.train(True)  # Set model to training mode
            else:
                model.train(False)  # Set model to evaluate mode

            running_loss = 0.0
            all_labels = []
            all_outputs = []

            # Iterate over data.
            data_loader = dataloaders[phase]
            for data in data_loader:
                # get the inputs
                inputs, labels = data

                # wrap them in Variable
                if use_gpu:
                    inputs = Variable(inputs.cuda(),requires_grad=True)
                    labels = Variable(labels.cuda())
                else:
                    inputs, labels = Variable(inputs), Variable(labels)

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                outputs = model(inputs)
                loss = criterion(outputs, labels)

                # backward + optimize only if in training phase
                if phase == 'train':
                    loss.backward()
                    optimizer.step()

                # statistics
                running_loss += loss.item() * inputs.size(0)
                all_labels.append(labels.cpu().numpy())
                all_outputs.append(outputs.detach().cpu().numpy())

            epoch_loss = running_loss / len(data_loader.dataset)
            all_labels = np.concatenate(all_labels, axis=0)
            all_outputs = np.concatenate(all_outputs, axis=0)

            # Calculate AUC for binary classification using the probability of class 1
            epoch_auc = roc_auc_score(all_labels, all_outputs[:, 1])

            print('{} Loss: {:.4f} AUC: {:.4f}'.format(
                phase, epoch_loss, epoch_auc))

            # deep copy the model
            if phase == 'val' and epoch_auc > best_auc:
                best_auc = epoch_auc
                best_model_wts = copy.deepcopy(model.state_dict())
        print()

    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(
        time_elapsed // 60, time_elapsed % 60))
    print('Best val AUC: {:4f}'.format(best_auc))

    # load best model weights
    model.load_state_dict(best_model_wts)
    return model

from open_clip import create_model_and_transforms
import utils

class ImageClassifier(torch.nn.Module):
    def __init__(self, image_encoder, classification_head, process_images=True):
        super().__init__()
        self.image_encoder = image_encoder
        self.classification_head = classification_head
        self.process_images = process_images
        if self.image_encoder is not None:
            self.train_preprocess = self.image_encoder.train_preprocess
            self.val_preprocess = self.image_encoder.val_preprocess

    def forward(self, inputs):
        if self.process_images:
            # inputs, _, _, _, _ = self.image_encoder(inputs)
            inputs = self.image_encoder(inputs)

        outputs = self.classification_head(inputs)
        return outputs

    def save(self, filename):
        print(f'Saving image classifier to {filename}')
        utils.torch_save(self, filename)

    @classmethod
    def load(cls, filename):
        print(f'Loading image classifier from {filename}')
        return utils.torch_load(filename)

class ImageEncoder(torch.nn.Module):
    def __init__(self, keep_lang=False):
        super().__init__()

        # self.model, self.train_preprocess, self.val_preprocess = clip.load(
        #     args.model, args.device, jit=False)
        self.model, self.train_preprocess, self.val_preprocess = create_model_and_transforms(
            model_name="coca_bioBert-ViT-B-32",
            pretrained='/home/work/austin/openclip/open_clip-main/src/logs/proposed_biobert_diffusion_full-grad-option/checkpoints/epoch_100.pt',
            # quick_gelu=True,
            device='cuda')

        # self.cache_dir = args.cache_dir

        if not keep_lang and hasattr(self.model, 'transformer'):
            delattr(self.model, 'transformer')

    def forward(self, images):
        assert self.model is not None
        return self.model.encode_image(images)

    def save(self, filename):
        print(f'Saving image encoder to {filename}')
        utils.torch_save(self, filename)

    @classmethod
    def load(cls, filename):
        print(f'Loading image encoder from {filename}')
        return utils.torch_load(filename)


class ClassificationHead(torch.nn.Linear):
    def __init__(self, normalize, biases=None):
        # output_size, input_size = weights.shape
        output_size = 2
        input_size = 768
        super().__init__(input_size, output_size)
        self.normalize = normalize
        # if weights is not None:
        #     self.weight = torch.nn.Parameter(weights.clone())
        if biases is not None:
            self.bias = torch.nn.Parameter(biases.clone())
        else:
            self.bias = torch.nn.Parameter(torch.zeros_like(self.bias))

    def forward(self, inputs):
        if self.normalize:
            inputs = inputs / inputs.norm(dim=-1, keepdim=True)
        return super().forward(inputs)

    def save(self, filename):
        print(f'Saving classification head to {filename}')
        utils.torch_save(self, filename)

    @classmethod
    def load(cls, filename):
        print(f'Loading classification head from {filename}')
        return utils.torch_load(filename)

def get_zeroshot_classifier(clip_model):
    classification_head = ClassificationHead(normalize=True)

    # classification_head = RegressionHead(input_size=512)

    return classification_head

image_encoder = ImageEncoder(keep_lang=True)
classification_head = get_zeroshot_classifier(image_encoder.model)
delattr(image_encoder.model, 'text_decoder')
delattr(image_encoder.model, 'text')
delattr(image_encoder.model, 'image_decoder')
# delattr(image_encoder.model, 'transformer')

model = ImageClassifier(image_encoder, classification_head, process_images=True)

device = torch.cuda.set_device(0)

model_ft = model

criterion = nn.CrossEntropyLoss()
model_ft = model_ft.cuda()

# Observe that all parameters are being optimized
optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-4, weight_decay=0.01)

# Decay LR by a factor of 0.1 every 7 epochs
exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)
since = time.time()

model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=10)

import pandas as pd

# Load the master CSV file
master_df = pd.read_csv('/home/work/austin/Dataset/mimic-cxr-jpg/2.0.0/master.csv')

# Define the base path for file paths
base_path = "/home/work/austin/Dataset/mimic-cxr-jpg/"

# Add filepath and title columns
master_df['filepath'] = base_path + master_df['Path']
# Replace newline characters in impression and findings before concatenating
master_df['impression'] = master_df['impression'].fillna('').str.replace('\n', ' ', regex=True)
master_df['findings'] = master_df['findings'].fillna('').str.replace('\n', ' ', regex=True)
master_df['title'] = master_df['impression'] + " " + master_df['findings']


# Filter data by split and select the necessary columns
train_df = master_df[master_df['split'] == 'train'][['filepath', 'title']]
validation_df = master_df[master_df['split'] == 'validate'][['filepath', 'title']]
test_df = master_df[master_df['split'] == 'test'][['filepath', 'title']]

# Save the filtered data to separate CSV files
train_df.to_csv('mgca_train.csv', sep='\t', index=False)
validation_df.to_csv('mgca_validation.csv',sep='\t',index=False)
test_df.to_csv('mgca_test.csv', sep='\t', index=False)

print("CSV files for train, validation, and test sets have been created.")

